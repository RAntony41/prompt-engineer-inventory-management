{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typhoon API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENTYPHOON_API_KEY = \"**ADD KEY**\"\n",
    "client = OpenAI(\n",
    "    api_key=OPENTYPHOON_API_KEY,\n",
    "    base_url=\"https://api.opentyphoon.ai/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 ms, sys: 9.41 ms, total: 22.8 ms\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"typhoon-v1.5-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are the best data scientist in inventory mangaement\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me an example of inventory management\"\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    temperature=0.8,\n",
    "    top_p=1,\n",
    "    stream=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, let\\'s consider a simple example of inventory management for an online retail store. There are multiple products that need to be tracked and managed. The goal is to maintain sufficient stock levels while minimizing storage costs, reducing waste, and maximizing sales.\\n\\nHere is a table with some example data:\\n| Product ID | Product Name | Quantity | Reorder Point | Safety Stock |\\n| --- | --- | --- | --- | --- |\\n| 1 | Smartphone | 10 | 5 | 3 |\\n| 2 | Tablet | 20 | 10 | 5 |\\n| 3 | Headphones | 15 | 8 | 4 |\\n\\nIn this table, \"Product ID\" is a unique identifier for each product, \"Product Name\" is the name of the product, \"Quantity\" is the current stock level, \"Reorder Point\" is the minimum stock level required before reordering, and \"Safety Stock\" is the buffer stock kept to protect against unexpected demand or delays in receiving new stock.\\n\\nTo manage the inventory effectively, the retailer needs to monitor the sales and reorder when the quantity falls below the reorder point. If the demand is uncertain or varies over time, it might be beneficial to implement more advanced inventory management techniques, such as forecasting or replenishment methods.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENTYPHOON_API_KEY = OPENTYPHOON_API_KEY\n",
    "base_url = \"https://api.opentyphoon.ai/v1\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"typhoon-v1.5-instruct\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key= OPENTYPHOON_API_KEY,\n",
    "    base_url= base_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1352b6790>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x134ce1490>, model_name='typhoon-v1.5-instruct', temperature=0.0, openai_api_key=SecretStr('**********'), openai_api_base='https://api.opentyphoon.ai/v1', openai_proxy='')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Thai\"),\n",
    "    HumanMessage(content=\"dog!\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'สุนัข!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchan Table Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pandas_code_Agent():\n",
    "    def __init__(self,dataframe,prefix_prompt,input_prompt) -> None:\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"typhoon-v1.5-instruct\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            api_key= OPENTYPHOON_API_KEY,\n",
    "            base_url= base_url,\n",
    "            )\n",
    "        self.dataframe = dataframe\n",
    "        self.prefix_prompt = prefix_prompt\n",
    "        self.input_prompt =input_prompt\n",
    "        self.agent = create_pandas_dataframe_agent(\n",
    "            self.llm,\n",
    "            self.dataframe,\n",
    "            verbose = True,\n",
    "            agent_type = AgentType.OPENAI_FUNCTIONS,\n",
    "            prefix = prefix_prompt,\n",
    "            allow_dangerous_code=True\n",
    "            )\n",
    "    \n",
    "    def create_code(self):\n",
    "        return self.agent.invoke(self.input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./stock_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_agent = pandas_code_Agent(dataframe=df,prefix_prompt=\"you are the best data sciantist, that give output ONLY A CODE\",\n",
    "                               input_prompt=\"Do feature extraction from time in this data frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo perform feature extraction on the time-related features in the given DataFrame, you can use the following Python code:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "\n",
      "# Assuming 'df' is your DataFrame\n",
      "df = pd.DataFrame({'day': [17, 10, 31, 29, 30],\n",
      "                   'onth': [6, 8, 7, 9, nan],\n",
      "                   'target': [0, 1.16162, 0, 0, 97.4578]})\n",
      "\n",
      "# Extracting year and month names\n",
      "df['year'] = df['month'].apply(lambda x: datetime(2000+x, 1, 1).year) \\\n",
      "   .fillna(method='ffill').astype(int)\n",
      "df['month_name'] = df['month'].apply(lambda x: datetime(2000+x, 1, 1).strftime('%B')) \\\n",
      "   .fillna(method='ffill')\n",
      "\n",
      "# Printing the updated DataFrame\n",
      "print(df)\n",
      "```\n",
      "\n",
      "This code will create a new DataFrame with additional columns for the extracted features: 'year' and 'onth_name'. The 'year' column is created by filling the missing values with the previous non-missing value's year. The 'onth_name' column is created by converting the month number to a string representation of the month name, filling the missing values with the previous non-missing value's month name.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = pandas_agent.create_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Do feature extraction from time in this data frame',\n",
       " 'output': \"To perform feature extraction on the time-related features in the given DataFrame, you can use the following Python code:\\n\\n```python\\nimport pandas as pd\\nfrom datetime import datetime\\n\\n# Assuming 'df' is your DataFrame\\ndf = pd.DataFrame({'day': [17, 10, 31, 29, 30],\\n                   'onth': [6, 8, 7, 9, nan],\\n                   'target': [0, 1.16162, 0, 0, 97.4578]})\\n\\n# Extracting year and month names\\ndf['year'] = df['month'].apply(lambda x: datetime(2000+x, 1, 1).year) \\\\\\n   .fillna(method='ffill').astype(int)\\ndf['month_name'] = df['month'].apply(lambda x: datetime(2000+x, 1, 1).strftime('%B')) \\\\\\n   .fillna(method='ffill')\\n\\n# Printing the updated DataFrame\\nprint(df)\\n```\\n\\nThis code will create a new DataFrame with additional columns for the extracted features: 'year' and 'onth_name'. The 'year' column is created by filling the missing values with the previous non-missing value's year. The 'onth_name' column is created by converting the month number to a string representation of the month name, filling the missing values with the previous non-missing value's month name.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To perform feature extraction on the time-related features in the given DataFrame, you can use the following Python code:\\n\\n```python\\nimport pandas as pd\\nfrom datetime import datetime\\n\\n# Assuming 'df' is your DataFrame\\ndf = pd.DataFrame({'day': [17, 10, 31, 29, 30],\\n                   'onth': [6, 8, 7, 9, nan],\\n                   'target': [0, 1.16162, 0, 0, 97.4578]})\\n\\n# Extracting year and month names\\ndf['year'] = df['month'].apply(lambda x: datetime(2000+x, 1, 1).year) \\\\\\n   .fillna(method='ffill').astype(int)\\ndf['month_name'] = df['month'].apply(lambda x: datetime(2000+x, 1, 1).strftime('%B')) \\\\\\n   .fillna(method='ffill')\\n\\n# Printing the updated DataFrame\\nprint(df)\\n```\\n\\nThis code will create a new DataFrame with additional columns for the extracted features: 'year' and 'onth_name'. The 'year' column is created by filling the missing values with the previous non-missing value's year. The 'onth_name' column is created by converting the month number to a string representation of the month name, filling the missing values with the previous non-missing value's month name.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class normal_Agent():\n",
    "    def __init__(self,system_prmopt) -> None:\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"typhoon-v1.5-instruct\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            api_key= OPENTYPHOON_API_KEY,\n",
    "            base_url= base_url,\n",
    "            )\n",
    "        self.system_prmopt = system_prmopt\n",
    "        \n",
    "    def create_code(self,user_prmopt): \n",
    "         return self.llm.invoke([\n",
    "            SystemMessage(content=self.system_prmopt),\n",
    "            HumanMessage(content=user_prmopt),\n",
    "            ]).content\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_agent = normal_Agent(\"\"\"From the text that I given. You have to give me only a feature extraction code.\n",
    "                            Input:\n",
    "                            The feature extraction code provided is written in Python using the Pandas library. It extracts two time-related features from the given DataFrame: 'year' and 'month_name'. Here's the code snippet again for reference:\n",
    "\n",
    "                            ```python\n",
    "                            import pandas as pd\n",
    "                            from datetime import datetime\n",
    "\n",
    "                            # Assuming 'df' is your DataFrame\n",
    "                            df = pd.DataFrame({'day': [17, 10, 31, 29, 30],\n",
    "                                               'onth': [6, 8, 7, 9, nan],\n",
    "                                               'target': [0, 1.16162, 0, 0, 97.4578]})\n",
    "\n",
    "                            # Extracting\n",
    "                            df['Weight (kg.)/Length (m.)'] = df['Weight (kg.)']/df['Length (m.)']\n",
    "                            df['thickness_mean'] df['thickness'].mean(\n",
    "\n",
    "                            # Printing the updated DataFrame\n",
    "                            print(df)\n",
    "                            ```\n",
    "                            \n",
    "                            Output:\n",
    "                            df['Weight (kg.)/Length (m.)'] = df['Weight (kg.)']/df['Length (m.)']\n",
    "                            df['thickness_mean'] df['thickness'].mean()\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for providing the feature extraction code. Based on the code you provided, I will now extract the relevant lines of code that perform the feature extraction.\n",
      "\n",
      "Here are the lines of code that extract the 'year' and 'month_name' features from the given DataFrame:\n",
      "\n",
      "```python\n",
      "df['year'] = df['month'].apply(lambda x: datetime(2000+x, 1, 1).year) \\\n",
      "   .fillna(method='ffill').astype(int)\n",
      "df['month_name'] = df['month'].apply(lambda x: datetime(2000+x, 1, 1).strftime('%B')) \\\n",
      "   .fillna(method='ffill')\n",
      "```\n",
      "\n",
      "These lines of code will create a new DataFrame with additional columns for the extracted features: 'year' and 'month_name'. The 'year' column is created by filling the missing values with the previous non-missing value's year. The 'month_name' column is created by converting the month number to a string representation of the month name, filling the missing values with the previous non-missing value's month name.\n"
     ]
    }
   ],
   "source": [
    "print(select_agent.create_code(answer['output']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data frame\n",
    "df = pd.read_csv('./stock_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data sciantist Agent\n",
    "pandas_agent_1 = pandas_code_Agent(dataframe=df,prefix_prompt=\"you are the best data sciantist, that expert in Machine Learning\",\n",
    "                               input_prompt=\"Do feature extraction from time in this data frame\")\n",
    "pandas_agent_2 = pandas_code_Agent(dataframe=df,prefix_prompt=\"you are the best data sciantist, that give output ONLY A CODE\",\n",
    "                               input_prompt=\"Do feature extraction from time in this data frame\")\n",
    "pandas_agent_3 = pandas_code_Agent(dataframe=df,prefix_prompt=\"you are the best data sciantist, that give output ONLY A CODE\",\n",
    "                               input_prompt=\"Do feature extraction from time in this data frame\")\n",
    "\n",
    "\n",
    "# Feature select Agent\n",
    "select_code_agent = normal_Agent(\"you are Machine learning Engineer\")\n",
    "\n",
    "\n",
    "\n",
    "# rule base to get all code\n",
    "\n",
    "#\n",
    "# select_feature_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data sciantist Agent\n",
    "pandas_agent_1 = pandas_code_Agent(dataframe=df,prefix_prompt=\"You are an expert data scientist with deep knowledge in feature engineering, data preprocessing, and machine learning. (GIVE ONLY CODE OUTPUT, USE ONLY NUMPY, PANDAS, SCIKIT-LEARN)\",\n",
    "                               input_prompt=\"Your task is to analyze the given dataset, represented as a pandas DataFrame from `pd.read_csv`, and extract the most relevant features to improve the performance of a machine learning model. Consider techniques such as removing irrelevant or redundant features, handling missing values, encoding categorical variables, and scaling numerical features.\")\n",
    "# Your task is to analyze the given dataset, represented as a pandas DataFrame from `pd.read_csv`, and extract the most relevant features to improve the performance of a machine learning model. \n",
    "# Consider techniques such as removing irrelevant or redundant features, handling missing values, encoding categorical variables, and scaling numerical features.\n",
    "pandas_agent_2 = pandas_code_Agent(dataframe=df,prefix_prompt=\"You are an expert data scientist with deep knowledge in feature engineering, data preprocessing, and machine learning. (GIVE ONLY CODE OUTPUT, USE ONLY NUMPY, PANDAS, SCIKIT-LEARN)\",\n",
    "                               input_prompt=\"Your task is to enhance the given dataset's usability for machine learning models by identifying the most important features and applying appropriate transformations. The dataset is loaded into a pandas DataFrame using `pd.read_csv`.\\nConsider the following steps:\\n1. **Correlation Analysis**: Calculate and analyze correlations between features to identify highly correlated pairs. Suggest removing or combining features with high collinearity.\\n2. **Dimensionality Reduction**: If applicable, apply dimensionality reduction techniques (e.g., PCA, LDA) to reduce the feature space while retaining the most informative features.\\n3. **Feature Creation**: Generate new features from existing ones by applying transformations (e.g., polynomial features, logarithmic transformations) that could potentially improve model performance.\")\n",
    "# Your task is to enhance the given dataset's usability for machine learning models by identifying the most important features and applying appropriate transformations. The dataset is loaded into a pandas DataFrame using `pd.read_csv`.\n",
    "# Consider the following steps:\n",
    "# 1. **Correlation Analysis**: Calculate and analyze correlations between features to identify highly correlated pairs. Suggest removing or combining features with high collinearity.\n",
    "# 2. **Dimensionality Reduction**: If applicable, apply dimensionality reduction techniques (e.g., PCA, LDA) to reduce the feature space while retaining the most informative features.\n",
    "# 3. **Feature Creation**: Generate new features from existing ones by applying transformations (e.g., polynomial features, logarithmic transformations) that could potentially improve model performance.\n",
    "pandas_agent_3 = pandas_code_Agent(dataframe=df,prefix_prompt=\"You are an expert data scientist with deep knowledge in feature engineering, data preprocessing, and machine learning. (GIVE ONLY CODE OUTPUT, USE ONLY NUMPY, PANDAS, SCIKIT-LEARN)\",\n",
    "                               input_prompt=\"Your task is to thoroughly clean and prepare the given dataset for feature extraction and machine learning model training. The dataset is loaded into a pandas DataFrame using `pd.read_csv`.\\nFollow these steps:\\n1. **Data Type Optimization**: Review the data types of all features and optimize them to reduce memory usage (e.g., convert object types to categories if they have a low cardinality).\\n2. **Outlier Detection and Treatment**: Detect and handle outliers in numerical features using appropriate methods (e.g., IQR, Z-score) to minimize their impact on the model.\\n3. **Normalization**: Normalize or standardize numerical features to bring them to a common scale, particularly if the data will be used with distance-based models or gradient descent methods.\\n4. **Feature Scaling for Different Model Types**: Provide suggestions on different scaling methods that may be more suitable for different types of machine learning models (e.g., tree-based vs. linear models).\")\n",
    "# Your task is to thoroughly clean and prepare the given dataset for feature extraction and machine learning model training. The dataset is loaded into a pandas DataFrame using `pd.read_csv`.\n",
    "# Follow these steps:\n",
    "# 1. **Data Type Optimization**: Review the data types of all features and optimize them to reduce memory usage (e.g., convert object types to categories if they have a low cardinality).\n",
    "# 2. **Outlier Detection and Treatment**: Detect and handle outliers in numerical features using appropriate methods (e.g., IQR, Z-score) to minimize their impact on the model.\n",
    "# 3. **Normalization**: Normalize or standardize numerical features to bring them to a common scale, particularly if the data will be used with distance-based models or gradient descent methods.\n",
    "# 4. **Feature Scaling for Different Model Types**: Provide suggestions on different scaling methods that may be more suitable for different types of machine learning models (e.g., tree-based vs. linear models).\n",
    "\n",
    "# Feature select Agent\n",
    "select_code_agent = normal_Agent(\"you are Machine learning Engineer\")\n",
    "\n",
    "\n",
    "\n",
    "# rule base to get all code\n",
    "\n",
    "#\n",
    "# select_feature_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFirst, let's import the necessary libraries and load the dataset. Then we will handle missing values, encode categorical variables, and scale numerical features. Finally, we will select the most relevant features based on their correlation with the target variable.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Handle missing values\n",
      "df.fillna(df.mean(), inplace=True)\n",
      "\n",
      "# Encode categorical variables\n",
      "le = LabelEncoder()\n",
      "df['ยี่ห้อ'] = le.fit_transform(df['ยี่ห้อ'])\n",
      "df['AZ'] = le.fit_transform(df['AZ'])\n",
      "df['ลูกที่'] = le.fit_transform(df['ลูกที่'])\n",
      "df['Colour'] = le.fit_transform(df['Colour'])\n",
      "df['day'] = le.fit_transform(df['day'])\n",
      "df['month'] = le.fit_transform(df['month'])\n",
      "\n",
      "# Scale numerical features\n",
      "scaler = StandardScaler()\n",
      "df[['Weight (kg.)', 'Length (m.)', 'kg./m.']] = scaler.fit_transform(df[['Weight (kg.)', 'Length (m.)', 'kg./m.']])\n",
      "\n",
      "# Select the most relevant features\n",
      "X = df.drop(['target'], axis=1)\n",
      "y = df['target']\n",
      "\n",
      "mutual_info = mutual_info_classif(X, y, random_state=0)\n",
      "chi2_scores = chi2(X, y, ddof=2)\n",
      "f_scores = f_classif(X, y, independent_var='all')\n",
      "\n",
      "feature_scores = dict(zip(X.columns, mutual_info))\n",
      "sorted_feature_scores = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
      "\n",
      "relevant_features = [column for column, score in sorted_feature_scores[:5]]\n",
      "\n",
      "print(\"The most relevant features are:\", relevant_features)\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Correlation Analysis\n",
      "corr_matrix = df.corr()\n",
      "print(corr_matrix)\n",
      "\n",
      "# Identify highly correlated pairs\n",
      "highly_correlated_pairs = [(corr_matrix.columns[i], corr_matrix.columns[j]) for i in range(len(corr_matrix.columns))\n",
      "                            for j in range(i) if abs(corr_matrix.iloc[i, j]) > 0.95]\n",
      "\n",
      "print(highly_correlated_pairs)\n",
      "\n",
      "# Dimensionality Reduction\n",
      "features = df.drop(['target'], axis=1)\n",
      "labels = df['target']\n",
      "\n",
      "scaler = StandardScaler()\n",
      "features_scaled = scaler.fit_transform(features)\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "features_reduced = pca.fit_transform(features_scaled)\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(features_reduced, labels)\n",
      "\n",
      "# Feature Creation\n",
      "df['new_feature'] = df['feature'].apply(lambda x: x**2)\n",
      "\n",
      "# Split dataset into training and testing sets\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(features_reduced, labels, test_size=0.2, random_state=42)\n",
      "\n",
      "# Train and evaluate the model\n",
      "model.fit(X_train, y_train)\n",
      "y_pred = model.predict(X_test)\n",
      "from sklearn.metrics import accuracy_score\n",
      "print(accuracy_score(y_test, y_pred))\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "# Step 1: Data Type Optimization\n",
      "df = df.astype('float64') # Convert object types to float64\n",
      "\n",
      "# Step 2: Outlier Detection and Treatment\n",
      "numerical_features = ['AZ', 'ลูกที่', 'thickness', 'Weight (kg.)', 'Length (m.)', 'kg./m.', 'day', 'onth']\n",
      "for col in numerical_features:\n",
      "    Q1 = df[col].quantile(0.25)\n",
      "    Q3 = df[col].quantile(0.75)\n",
      "    IQR = Q3 - Q1\n",
      "    \n",
      "    # Handle outliers with Z-score method\n",
      "    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
      "    df.loc[z_scores > 3, col] = np.nan\n",
      "    \n",
      "    # Impute missing values with median\n",
      "    df[col].fillna(df[col].median(), inplace=True)\n",
      "\n",
      "# Step 3: Normalization\n",
      "scaler = StandardScaler()\n",
      "df[['AZ', 'ลูกที่', 'thickness', 'Weight (kg.)', 'Length (m.)', 'kg./m.'] = scaler.fit_transform(df[['AZ', 'ลูกที่', 'thickness', 'Weight (kg.)', 'Length (m.)', 'kg./m.']])\n",
      "\n",
      "# Step 4: Feature Scaling for Different Model Types\n",
      "# For tree-based models, use log transformation\n",
      "df['AZ_log'] = np.log(df['AZ'])\n",
      "df['ลูกที่_log'] = np.log(df['ลูกที่'])\n",
      "\n",
      "# For linear models, use min-max scaling\n",
      "linear_scaler = MinMaxScaler()\n",
      "df[['AZ_linear', 'ลูกที่_linear']] = linear_scaler.fit_transform(df[['AZ', 'ลูกที่']])\n",
      "\n",
      "# Split the dataset into input features (X) and target variable (y)\n",
      "X = df.drop(['target'], axis=1)\n",
      "y = df['target']\n",
      "\n",
      "# Train-test split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Define a pipeline for model training\n",
      "pipeline = Pipeline([\n",
      "    ('column_transformer', ColumnTransformer([\n",
      "        ('\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: user 981 ms, sys: 111 ms, total: 1.09 s\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "answer1 = pandas_agent_1.create_code()\n",
    "answer2 = pandas_agent_2.create_code()\n",
    "answer3 = pandas_agent_3.create_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, let's import the necessary libraries and load the dataset. Then we will handle missing values, encode categorical variables, and scale numerical features. Finally, we will select the most relevant features based on their correlation with the target variable.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Handle missing values\n",
      "df.fillna(df.mean(), inplace=True)\n",
      "\n",
      "# Encode categorical variables\n",
      "le = LabelEncoder()\n",
      "df['ยี่ห้อ'] = le.fit_transform(df['ยี่ห้อ'])\n",
      "df['AZ'] = le.fit_transform(df['AZ'])\n",
      "df['ลูกที่'] = le.fit_transform(df['ลูกที่'])\n",
      "df['Colour'] = le.fit_transform(df['Colour'])\n",
      "df['day'] = le.fit_transform(df['day'])\n",
      "df['month'] = le.fit_transform(df['month'])\n",
      "\n",
      "# Scale numerical features\n",
      "scaler = StandardScaler()\n",
      "df[['Weight (kg.)', 'Length (m.)', 'kg./m.']] = scaler.fit_transform(df[['Weight (kg.)', 'Length (m.)', 'kg./m.']])\n",
      "\n",
      "# Select the most relevant features\n",
      "X = df.drop(['target'], axis=1)\n",
      "y = df['target']\n",
      "\n",
      "mutual_info = mutual_info_classif(X, y, random_state=0)\n",
      "chi2_scores = chi2(X, y, ddof=2)\n",
      "f_scores = f_classif(X, y, independent_var='all')\n",
      "\n",
      "feature_scores = dict(zip(X.columns, mutual_info))\n",
      "sorted_feature_scores = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
      "\n",
      "relevant_features = [column for column, score in sorted_feature_scores[:5]]\n",
      "\n",
      "print(\"The most relevant features are:\", relevant_features)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(answer1['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant features are: ['ยี่ห้อ', 'AZ', 'ลูกที่', 'Colour', 'day']\n"
     ]
    }
   ],
   "source": [
    "print(select_agent.create_code(answer1['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Correlation Analysis\n",
      "corr_matrix = df.corr()\n",
      "print(corr_matrix)\n",
      "\n",
      "# Identify highly correlated pairs\n",
      "highly_correlated_pairs = [(corr_matrix.columns[i], corr_matrix.columns[j]) for i in range(len(corr_matrix.columns))\n",
      "                            for j in range(i) if abs(corr_matrix.iloc[i, j]) > 0.95]\n",
      "\n",
      "print(highly_correlated_pairs)\n",
      "\n",
      "# Dimensionality Reduction\n",
      "features = df.drop(['target'], axis=1)\n",
      "labels = df['target']\n",
      "\n",
      "scaler = StandardScaler()\n",
      "features_scaled = scaler.fit_transform(features)\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "features_reduced = pca.fit_transform(features_scaled)\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(features_reduced, labels)\n",
      "\n",
      "# Feature Creation\n",
      "df['new_feature'] = df['feature'].apply(lambda x: x**2)\n",
      "\n",
      "# Split dataset into training and testing sets\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(features_reduced, labels, test_size=0.2, random_state=42)\n",
      "\n",
      "# Train and evaluate the model\n",
      "model.fit(X_train, y_train)\n",
      "y_pred = model.predict(X_test)\n",
      "from sklearn.metrics import accuracy_score\n",
      "print(accuracy_score(y_test, y_pred))\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(answer2['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature extraction code provided is written in Python using the Pandas, NumPy, and Scikit-learn libraries. It performs the following tasks:\n",
      "\n",
      "1. Loads a dataset from a CSV file named 'data.csv'.\n",
      "2. Calculates the correlation matrix of the dataset.\n",
      "3. Identifies highly correlated pairs of features with a correlation coefficient greater than 0.95.\n",
      "4. Performs dimensionality reduction using Principal Component Analysis (PCA) with 2 components.\n",
      "5. Scales the data using StandardScaler.\n",
      "6. Trains a logistic regression model on the reduced and scaled features.\n",
      "7. Creates a new feature by squaring an existing feature.\n",
      "8. Splits the dataset into training and testing sets with a test size of 0.2.\n",
      "9. Evaluates the model's performance on the testing set using the accuracy score metric.\n"
     ]
    }
   ],
   "source": [
    "print(select_agent.create_code(answer2['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "# Step 1: Data Type Optimization\n",
      "df = df.astype('float64') # Convert object types to float64\n",
      "\n",
      "# Step 2: Outlier Detection and Treatment\n",
      "numerical_features = ['AZ', 'ลูกที่', 'thickness', 'Weight (kg.)', 'Length (m.)', 'kg./m.', 'day', 'onth']\n",
      "for col in numerical_features:\n",
      "    Q1 = df[col].quantile(0.25)\n",
      "    Q3 = df[col].quantile(0.75)\n",
      "    IQR = Q3 - Q1\n",
      "    \n",
      "    # Handle outliers with Z-score method\n",
      "    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
      "    df.loc[z_scores > 3, col] = np.nan\n",
      "    \n",
      "    # Impute missing values with median\n",
      "    df[col].fillna(df[col].median(), inplace=True)\n",
      "\n",
      "# Step 3: Normalization\n",
      "scaler = StandardScaler()\n",
      "df[['AZ', 'ลูกที่', 'thickness', 'Weight (kg.)', 'Length (m.)', 'kg./m.'] = scaler.fit_transform(df[['AZ', 'ลูกที่', 'thickness', 'Weight (kg.)', 'Length (m.)', 'kg./m.']])\n",
      "\n",
      "# Step 4: Feature Scaling for Different Model Types\n",
      "# For tree-based models, use log transformation\n",
      "df['AZ_log'] = np.log(df['AZ'])\n",
      "df['ลูกที่_log'] = np.log(df['ลูกที่'])\n",
      "\n",
      "# For linear models, use min-max scaling\n",
      "linear_scaler = MinMaxScaler()\n",
      "df[['AZ_linear', 'ลูกที่_linear']] = linear_scaler.fit_transform(df[['AZ', 'ลูกที่']])\n",
      "\n",
      "# Split the dataset into input features (X) and target variable (y)\n",
      "X = df.drop(['target'], axis=1)\n",
      "y = df['target']\n",
      "\n",
      "# Train-test split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Define a pipeline for model training\n",
      "pipeline = Pipeline([\n",
      "    ('column_transformer', ColumnTransformer([\n",
      "        ('\n"
     ]
    }
   ],
   "source": [
    "print(answer3['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "        ('imputer', SimpleImputer(strategy='median'), ['AZ', 'ลูกที่', 'thickness', 'Weight (kg.)', 'Length (m.)', 'kg./m.']),\n",
      "        ('scaler', scaler, ['AZ', 'ลูกที่', 'thickness', 'Weight (kg.)', 'Length (m.)', 'kg./m.']),\n",
      "        ('encoder', StandardScaler(), ['day', 'onth'])\n",
      "    ])\n",
      "],\n",
      "    ('model', Pipeline([\n",
      "        ('feature_selection', SelectKBest(chi2, k=5)),\n",
      "        ('regression', LinearRegression())\n",
      "    ]))\n",
      ")\n",
      "\n",
      "# Fit the pipeline to the training data\n",
      "pipeline.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions on the test set\n",
      "y_pred = pipeline.predict(X_test)\n",
      "\n",
      "# Evaluate the model\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
      "print(f\"RMSE: {rmse:.2f}\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(select_agent.create_code(answer3['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
